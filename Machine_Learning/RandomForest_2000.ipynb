{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9041a129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hpi_type</th>\n",
       "      <th>hpi_flavor</th>\n",
       "      <th>frequency</th>\n",
       "      <th>level</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_id</th>\n",
       "      <th>yr</th>\n",
       "      <th>period</th>\n",
       "      <th>index_nsa</th>\n",
       "      <th>index_sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>1</td>\n",
       "      <td>149.78</td>\n",
       "      <td>150.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>2</td>\n",
       "      <td>149.29</td>\n",
       "      <td>149.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>3</td>\n",
       "      <td>151.34</td>\n",
       "      <td>150.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>4</td>\n",
       "      <td>152.36</td>\n",
       "      <td>151.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>5</td>\n",
       "      <td>153.57</td>\n",
       "      <td>151.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     hpi_type hpi_flavor               frequency  \\\n",
       "0           0  traditional    monthly  USA or Census Division   \n",
       "1           1  traditional    monthly  USA or Census Division   \n",
       "2           2  traditional    monthly  USA or Census Division   \n",
       "3           3  traditional    monthly  USA or Census Division   \n",
       "4           4  traditional    monthly  USA or Census Division   \n",
       "\n",
       "                         level place_name  place_id             yr  period  \\\n",
       "0  East North Central Division     DV_ENC      2000  purchase-only       1   \n",
       "1  East North Central Division     DV_ENC      2000  purchase-only       2   \n",
       "2  East North Central Division     DV_ENC      2000  purchase-only       3   \n",
       "3  East North Central Division     DV_ENC      2000  purchase-only       4   \n",
       "4  East North Central Division     DV_ENC      2000  purchase-only       5   \n",
       "\n",
       "   index_nsa  index_sa  \n",
       "0     149.78    150.22  \n",
       "1     149.29    149.33  \n",
       "2     151.34    150.79  \n",
       "3     152.36    151.47  \n",
       "4     153.57    151.82  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "df_2000 = pd.read_csv('../data_csv/clean_2000_df.csv')\n",
    "df_2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758ea135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpi_type</th>\n",
       "      <th>hpi_flavor</th>\n",
       "      <th>frequency</th>\n",
       "      <th>level</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_id</th>\n",
       "      <th>yr</th>\n",
       "      <th>period</th>\n",
       "      <th>index_nsa</th>\n",
       "      <th>index_sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>1</td>\n",
       "      <td>149.78</td>\n",
       "      <td>150.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>2</td>\n",
       "      <td>149.29</td>\n",
       "      <td>149.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>3</td>\n",
       "      <td>151.34</td>\n",
       "      <td>150.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>4</td>\n",
       "      <td>152.36</td>\n",
       "      <td>151.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>traditional</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>2000</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>5</td>\n",
       "      <td>153.57</td>\n",
       "      <td>151.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hpi_type hpi_flavor               frequency  \\\n",
       "0  traditional    monthly  USA or Census Division   \n",
       "1  traditional    monthly  USA or Census Division   \n",
       "2  traditional    monthly  USA or Census Division   \n",
       "3  traditional    monthly  USA or Census Division   \n",
       "4  traditional    monthly  USA or Census Division   \n",
       "\n",
       "                         level place_name  place_id             yr  period  \\\n",
       "0  East North Central Division     DV_ENC      2000  purchase-only       1   \n",
       "1  East North Central Division     DV_ENC      2000  purchase-only       2   \n",
       "2  East North Central Division     DV_ENC      2000  purchase-only       3   \n",
       "3  East North Central Division     DV_ENC      2000  purchase-only       4   \n",
       "4  East North Central Division     DV_ENC      2000  purchase-only       5   \n",
       "\n",
       "   index_nsa  index_sa  \n",
       "0     149.78    150.22  \n",
       "1     149.29    149.33  \n",
       "2     151.34    150.79  \n",
       "3     152.36    151.47  \n",
       "4     153.57    151.82  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the non-beneficial ID columns\n",
    "df_2000 = df_2000.drop(columns = ['Unnamed: 0'], axis= 1)\n",
    "df_2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2196579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hpi_type        3\n",
       "hpi_flavor      2\n",
       "frequency       4\n",
       "level         162\n",
       "place_name    162\n",
       "yr              2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for bucketing\n",
    "# Generate our categorical variable list\n",
    "cat_2000 = df_2000.dtypes[df_2000.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "# Check the number of unique values in each column\n",
    "df_2000[cat_2000].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3bfd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "East North Central Division    20\n",
       "Pacific Division               20\n",
       "United States                  20\n",
       "West North Central Division    20\n",
       "South Atlantic Division        20\n",
       "                               ..\n",
       "Fresno, CA                      4\n",
       "Gary, IN (MSAD)                 4\n",
       "Grand Rapids-Kentwood, MI       4\n",
       "Greensboro-High Point, NC       4\n",
       "Puerto Rico                     4\n",
       "Name: level, Length: 162, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#11 uniques values needs to be checked \n",
    "# Check the unique value counts to see if binning is required\n",
    "df_2000.level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8358ec58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpi_type_developmental</th>\n",
       "      <th>hpi_type_distress-free</th>\n",
       "      <th>hpi_type_traditional</th>\n",
       "      <th>hpi_flavor_monthly</th>\n",
       "      <th>hpi_flavor_quarterly</th>\n",
       "      <th>frequency_MSA</th>\n",
       "      <th>frequency_Puerto Rico</th>\n",
       "      <th>frequency_State</th>\n",
       "      <th>frequency_USA or Census Division</th>\n",
       "      <th>level_Akron, OH</th>\n",
       "      <th>...</th>\n",
       "      <th>place_name_USA</th>\n",
       "      <th>place_name_UT</th>\n",
       "      <th>place_name_VA</th>\n",
       "      <th>place_name_VT</th>\n",
       "      <th>place_name_WA</th>\n",
       "      <th>place_name_WI</th>\n",
       "      <th>place_name_WV</th>\n",
       "      <th>place_name_WY</th>\n",
       "      <th>yr_expanded-data</th>\n",
       "      <th>yr_purchase-only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 335 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hpi_type_developmental  hpi_type_distress-free  hpi_type_traditional  \\\n",
       "0                     0.0                     0.0                   1.0   \n",
       "1                     0.0                     0.0                   1.0   \n",
       "2                     0.0                     0.0                   1.0   \n",
       "3                     0.0                     0.0                   1.0   \n",
       "4                     0.0                     0.0                   1.0   \n",
       "\n",
       "   hpi_flavor_monthly  hpi_flavor_quarterly  frequency_MSA  \\\n",
       "0                 1.0                   0.0            0.0   \n",
       "1                 1.0                   0.0            0.0   \n",
       "2                 1.0                   0.0            0.0   \n",
       "3                 1.0                   0.0            0.0   \n",
       "4                 1.0                   0.0            0.0   \n",
       "\n",
       "   frequency_Puerto Rico  frequency_State  frequency_USA or Census Division  \\\n",
       "0                    0.0              0.0                               1.0   \n",
       "1                    0.0              0.0                               1.0   \n",
       "2                    0.0              0.0                               1.0   \n",
       "3                    0.0              0.0                               1.0   \n",
       "4                    0.0              0.0                               1.0   \n",
       "\n",
       "   level_Akron, OH  ...  place_name_USA  place_name_UT  place_name_VA  \\\n",
       "0              0.0  ...             0.0            0.0            0.0   \n",
       "1              0.0  ...             0.0            0.0            0.0   \n",
       "2              0.0  ...             0.0            0.0            0.0   \n",
       "3              0.0  ...             0.0            0.0            0.0   \n",
       "4              0.0  ...             0.0            0.0            0.0   \n",
       "\n",
       "   place_name_VT  place_name_WA  place_name_WI  place_name_WV  place_name_WY  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   yr_expanded-data  yr_purchase-only  \n",
       "0               0.0               1.0  \n",
       "1               0.0               1.0  \n",
       "2               0.0               1.0  \n",
       "3               0.0               1.0  \n",
       "4               0.0               1.0  \n",
       "\n",
       "[5 rows x 335 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are a substantial number of datapoints. leave it alone no need to bucket\n",
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(df_2000[cat_2000]))\n",
    "\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encode_df.columns = enc.get_feature_names(cat_2000)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ef8537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseperez/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place_id</th>\n",
       "      <th>period</th>\n",
       "      <th>index_nsa</th>\n",
       "      <th>index_sa</th>\n",
       "      <th>hpi_type_developmental</th>\n",
       "      <th>hpi_type_distress-free</th>\n",
       "      <th>hpi_type_traditional</th>\n",
       "      <th>hpi_flavor_monthly</th>\n",
       "      <th>hpi_flavor_quarterly</th>\n",
       "      <th>frequency_MSA</th>\n",
       "      <th>...</th>\n",
       "      <th>place_name_USA</th>\n",
       "      <th>place_name_UT</th>\n",
       "      <th>place_name_VA</th>\n",
       "      <th>place_name_VT</th>\n",
       "      <th>place_name_WA</th>\n",
       "      <th>place_name_WI</th>\n",
       "      <th>place_name_WV</th>\n",
       "      <th>place_name_WY</th>\n",
       "      <th>yr_expanded-data</th>\n",
       "      <th>yr_purchase-only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>149.78</td>\n",
       "      <td>150.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>149.29</td>\n",
       "      <td>149.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>151.34</td>\n",
       "      <td>150.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>4</td>\n",
       "      <td>152.36</td>\n",
       "      <td>151.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "      <td>153.57</td>\n",
       "      <td>151.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 339 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   place_id  period  index_nsa  index_sa  hpi_type_developmental  \\\n",
       "0      2000       1     149.78    150.22                     0.0   \n",
       "1      2000       2     149.29    149.33                     0.0   \n",
       "2      2000       3     151.34    150.79                     0.0   \n",
       "3      2000       4     152.36    151.47                     0.0   \n",
       "4      2000       5     153.57    151.82                     0.0   \n",
       "\n",
       "   hpi_type_distress-free  hpi_type_traditional  hpi_flavor_monthly  \\\n",
       "0                     0.0                   1.0                 1.0   \n",
       "1                     0.0                   1.0                 1.0   \n",
       "2                     0.0                   1.0                 1.0   \n",
       "3                     0.0                   1.0                 1.0   \n",
       "4                     0.0                   1.0                 1.0   \n",
       "\n",
       "   hpi_flavor_quarterly  frequency_MSA  ...  place_name_USA  place_name_UT  \\\n",
       "0                   0.0            0.0  ...             0.0            0.0   \n",
       "1                   0.0            0.0  ...             0.0            0.0   \n",
       "2                   0.0            0.0  ...             0.0            0.0   \n",
       "3                   0.0            0.0  ...             0.0            0.0   \n",
       "4                   0.0            0.0  ...             0.0            0.0   \n",
       "\n",
       "   place_name_VA  place_name_VT  place_name_WA  place_name_WI  place_name_WV  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   place_name_WY  yr_expanded-data  yr_purchase-only  \n",
       "0            0.0               0.0               1.0  \n",
       "1            0.0               0.0               1.0  \n",
       "2            0.0               0.0               1.0  \n",
       "3            0.0               0.0               1.0  \n",
       "4            0.0               0.0               1.0  \n",
       "\n",
       "[5 rows x 339 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "df_2000 = df_2000.merge(encode_df,left_index=True, right_index=True)\n",
    "df_2000 = df_2000.drop(cat_2000,1)\n",
    "df_2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0abb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove loan status target from features data\n",
    "y = df_2000.hpi_type_traditional\n",
    "X = df_2000.drop(columns=[\"index_sa\",\"index_nsa\"])\n",
    "\n",
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427faa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random forest predictive accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "857604a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-22 17:23:09.577140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 3ms/step - loss: 0.7487 - accuracy: 0.5651\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2322 - accuracy: 0.9481\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9884\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9958\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0352 - accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 8.8561e-04 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 7.8081e-04 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 6.9685e-04 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 6.2409e-04 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 5.6376e-04 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 5.1219e-04 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 4.6697e-04 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 4.2766e-04 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 3.9020e-04 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 3.5781e-04 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 3.2972e-04 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 3.0510e-04 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 2.8247e-04 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 2.6264e-04 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 2.4483e-04 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 2.2910e-04 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 2.1411e-04 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 2.0066e-04 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.8880e-04 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.7779e-04 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.6788e-04 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.5869e-04 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.5018e-04 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4241e-04 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3505e-04 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.2837e-04 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.2202e-04 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.1613e-04 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.1065e-04 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.0552e-04 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.0061e-04 - accuracy: 1.0000\n",
      "10/10 - 0s - loss: 8.8668e-05 - accuracy: 1.0000 - 177ms/epoch - 18ms/step\n",
      "Loss: 8.866780990501866e-05, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 24\n",
    "hidden_nodes_layer2 = 12\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4aeff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
